<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>EmojiDiff</title>
  <link href="./static/css/style.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation </strong> </h1>
  <p id="authors"><a href="https://janspiry.github.io/">Liangwei Jiang</a> <a href="https://www.semanticscholar.org/author/Ruida-Li/2288749443">Ruida Li</a>  <a href="https://scholar.google.com/citations?hl=zh-CN&user=0BSwA78AAAAJ">Zhifeng Zhang</a>  <a href="https://www.semanticscholar.org/author/Shuo-Fang/2289607048">Shuo Fang</a>  <a href="https://www.researchgate.net/profile/Chenguang-Ma">Chenguang Ma</a> <br>
    <span style="font-size: 16px"><br>
      Terminal Technology Department, Alipay, Ant Group.
    </span>
        </p>
    <div class="column has-text-centered">
      <div class="publication-links">
        <span class="link-block">
          <a href="https://arxiv.org/abs/2412.01254"
              class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
        <!-- Code Link. -->
        <span class="link-block">
          <a href=""
              class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="fab fa-github"></i>
            </span>
            <span>Code (Coming soon)</span>
            </a>
        </span>
        <span class="link-block">
          <a href=""
              class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="far fa-images"></i>
            </span>
            <span>Data</span>
            </a>
        </span>
      </div>
    </div>
    <b>EmojiDiff</b> is an end-to-end solution that integrates fine-grained expression control, high-fidelity ID preservation, and strong adaptability to various diffusion models.
    <img class="summary-img" src="assets/bannber.svg" style="width:50%;"> <br>
</div>

<div class="content">
  <h2 style="text-align:center;"><strong>Abstract</strong></h2>
  <p>

    This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity. On one hand, fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait. On the other hand, even coarse-grained expression control can cause facial changes that compromise identity, since they all act on the face. These limitations remain unaddressed by previous generation methods, which primarily rely on coarse control signals or two-stage inference that integrates portrait animation. Here, we introduce <b>EmojiDiff</b>, the first end-to-end solution that enables simultaneous control of extremely detailed expression (RGB-level) and high-fidelity identity in portrait generation. To address the above challenges, EmojiDiff adopts a two-stage scheme involving decoupled training and fine-tuning. For decoupled training, we innovate <b>I</b>D-irrelevant <b>D</b>ata <b>I</b>teration (IDI) to synthesize cross-identity expression pairs by separately manipulating the expression and identity, achieving a stable and high-quality data generation. Training the model with this data, we effectively disentangle fine expression features in the expression template from other extraneous information (\textit{e.g.}, identity, skin). Subsequently, we present <b>I</b>D-enhanced <b>C</b>ontrast <b>A</b>lignment (ICA) for further fine-tuning. ICA achieves rapid reconstruction and joint supervision of identity and expression information, thus aligning identity representations of images with and without expression control. Experimental results demonstrate that our method remarkably outperforms counterparts, achieves precise expression control with highly maintained identity, and generalizes well to various diffusion models.

  </p>
  </div>


<div class="content">
    <h2 style="text-align:center;"><strong>Method</strong></h2>
    <p>
      To integrate RGB-driven expression control into diffusion models, we aim to synthesize cross-identity data for the model's decoupled training, and mitigate the negative impact on the original identity through contrastive alignment fine-tuning. Before decoupled training, the fundamental expression controller (i.e., Base E-Adapter) is trained with same-identity triplet data. Next, the trained Base E-Adapter and FaceFusion are utilized to alter the identity of portraits while maintaining consistent expressions, thereby creating cross-identity expression pairs. Subsequently, the Refined E-Adapter uses newly synthesized data for disentangled training, facilitating dual control of identity and expression without ID leakage. Finally, the Refined E-Adapter is fine-tuned by expression and identity loss based on ANI.
    </p>
    <br>
    <img class="summary-img" src="assets/pipeline.svg" style="width:90%;"> <br>
  </div>

<div class="content">
  <h2 style="text-align:center;"><strong>Comparison Results</strong></h2>
  <img src="assets/compare.svg" class="teaser-gif" style="width:100%;">
</div>


<div class="content">
  <h2 style="text-align:center;"><strong> Gallery</strong></h2>
    <h4>SD1.5-based Results</h4>
    <img class="summary-img" src="assets/sd15.svg" style="width:100%;">
    <h4>SDXL-based Results</h4>
    <img class="summary-img" src="assets/sdxl.svg" style="width:100%;">
    <h4>Dataset</h4>
    <img class="summary-img" src="assets/dataset.svg" style="width:100%;">
</div>
<div class="content">
  <h2 style="text-align:center; margin: 0 auto;"><strong>BibTex</strong></h2>
  <code>@article{jiang2024emojidiff,<br>
      title={EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation},<br> 
      author={Liangwei Jiang, Ruida Li, Zhifeng Zhang, Shuo Fang, Chenguang Ma},<br>
      journal={arXiv preprint arXiv:2412.01254},<br>
      year={2024},<br>
}
   </code>
</div>

</body>


</html>
