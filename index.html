<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>EmojiDiff</title>
  <link href="./static/css/style.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation </strong> </h1>
  <p id="authors"><a href="https://janspiry.github.io/">Liangwei Jiang</a> <a href="https://www.semanticscholar.org/author/Ruida-Li/2288749443">Ruida Li</a>  <a href="https://scholar.google.com/citations?hl=zh-CN&user=0BSwA78AAAAJ">Zhifeng Zhang</a>  <a href="https://www.semanticscholar.org/author/Shuo-Fang/2289607048">Shuo Fang</a>  <a href="https://www.researchgate.net/profile/Chenguang-Ma">Chenguang Ma</a> <br>
    <span style="font-size: 16px"><br>
      Terminal Technology Department, Alipay, Ant Group.
    </span>
        </p>
    <div class="column has-text-centered">
      <div class="publication-links">
        <span class="link-block">
          <a href="https://arxiv.org/abs/2412.01254"
              class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
        <!-- Code Link. -->
        <span class="link-block">
          <a href=""
              class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <i class="fab fa-github"></i>
            </span>
            <span>Code (TBD)</span>
            </a>
        </span>
      </div>
    </div>
    <b>EmojiDiff</b> is an end-to-end solution that integrates fine-grained expression control, high-fidelity ID preservation, and strong adaptability to various diffusion models.
    <img class="summary-img" src="assets/bannber.svg" style="width:50%;"> <br>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>Abstract</strong></h2>
  <p>This paper aims to bring fine-grained expression control to identity-preserving portrait generation. Existing methods tend to synthesize portraits with either neutral or stereotypical expressions. Even when supplemented with control signals like facial landmarks, these models struggle to generate accurate and vivid expressions following user instructions.
    To solve this, we introduce <b>EmojiDiff</b>, an end-to-end solution to facilitate simultaneous dual control of fine expression and identity. Unlike the conventional methods using coarse control signals, our method directly accepts RGB expression images as input templates to provide extremely accurate and fine-grained expression control in the diffusion process.
    As its core, an innovative decoupled scheme is proposed to disentangle expression features in the expression template from other extraneous information, such as identity, skin, and style. On one hand, we introduce <b>I</b>D-irrelevant <b>D</b>ata <b>I</b>teration (IDI) to synthesize extremely high-quality cross-identity expression pairs for decoupled training,  which is the crucial foundation to filter out identity information hidden in the expressions.
    On the other hand, we meticulously investigate network layer function and select expression-sensitive layers to inject reference expression features, effectively preventing style leakage from expression signals. 
    To further improve identity fidelity, we propose a novel fine-tuning strategy named <b>I</b>D-enhanced <b>C</b>ontrast <b>A</b>lignment (ICA), which eliminates the negative impact of expression control on original identity preservation.
    Experimental results demonstrate that our method remarkably outperforms counterparts, achieves precise expression control with highly maintained identity, and generalizes well to various diffusion models.</p>
  </div>


<div class="content">
    <h2 style="text-align:center;"><strong>Method</strong></h2>
    <p> To integrate expression control into diffusion models, EmojiDiff aims to train the model using cross-identity triplet data and mitigate the negative impact on the original structure through contrastive alignment. The method involves four stages. First, the fundamental expression controller (i.e., Base E-Adapter) is trained with same-identity triplet data. Next, the trained Base E-Adapter and FaceFusion are utilized to alter the identity of portraits while maintaining consistent expressions, thereby creating cross-identity expression pairs. Subsequently, the Refined E-Adapter is trained using newly synthesized data, facilitating dual control of identity and expression without ID leakage. Finally, the Refined E-Adapter is fine-tuned by introducing expression and identity loss, further minimizing its negative impact on identity.</p>
    <br>
    <img class="summary-img" src="assets/pipeline.svg" style="width:90%;"> <br>
  </div>

<div class="content">
  <h2 style="text-align:center;"><strong>Comparison Results</strong></h2>
  <img src="assets/compare.svg" class="teaser-gif" style="width:100%;">
</div>


<div class="content">
  <h2 style="text-align:center;"><strong> Gallery</strong></h2>
    <h4>SD1.5-based Results</h4>
    <img class="summary-img" src="assets/sd15.svg" style="width:100%;">
    <h4>SDXL-based Results</h4>
    <img class="summary-img" src="assets/sdxl.svg" style="width:100%;">
    <h4>Dataset</h4>
    <img class="summary-img" src="assets/dataset.svg" style="width:100%;">
</div>
<div class="content">
  <h2 style="text-align:center; margin: 0 auto;"><strong>BibTex</strong></h2>
  <code>@article{jiang2024emojidiff,<br>
      title={EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation},<br> 
      author={Liangwei Jiang, Ruida Li, Zhifeng Zhang, Shuo Fang, Chenguang Ma},<br>
      journal={arXiv preprint arXiv:2412.01254},<br>
      year={2024},<br>
}
   </code>
</div>

</body>


</html>
